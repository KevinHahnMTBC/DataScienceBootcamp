{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.4: Challenge (Logistic, Ridge, LASSO Regression)\n",
    "## Kevin Hahn  \n",
    "\n",
    "<b>In your report, evaluate all three models and decide on your best. Be clear about the decisions you made that led to these models (feature selection, regularization parameter selection, model evaluation criteria) and why you think that particular model is the best of the three. Also reflect on the strengths and limitations of regression as a modeling approach. Were there things you couldn't do but you wish you could have done?\n",
    "\n",
    "Record your work and reflections in a notebook to discuss with your mentor.</b>\n",
    "\n",
    "I looked at the Wisconsin breast cancer dataset (http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29) and ran a standard logistic regression, ridge regression, and LASSO regression on the original 9 predictor variables (\"few features\") and also those original with 6 added features (\"many features\"). Features were formed by finding products of predictors strongly correlated with one another or with the target variable of stage 2/stage 4 cancer. L2 regularization was used for the \"vanilla\" logistic regression as least squares regression appeared to account for most variance in the target variable. \n",
    "\n",
    "Here were the R-squared values for the few and many feature set versions of the three regressions:\n",
    "\n",
    "\"Vanilla\" logistic regression with few features, with many features:\n",
    "0.985380116959, 0.753461134895\n",
    "\n",
    "Ridge logistic regression with few features, with many features:\n",
    "0.730833757103, 0.797642266828\n",
    "\n",
    "LASSO logistic regression with few features, with many features:\n",
    "0.722511709555, 0.753461134895\n",
    "\n",
    "Standard logistic regression with the naturally-occurring features already present in the dataset seemed to account for the most variance. The high R-squared value found in the test set seems to parallel the accuracy of the original Logit regression model. \n",
    "\n",
    "I do wish I knew more about the dataset itself because it seems like some of the values assigned are scaled linearly or ordinally but it's hard to say if some of the values assigned are subjective or which features/predictor variables could further be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression R² for the test data with few features:\n",
      "0.985380116959\n",
      "\n",
      "Logistic Regression R² for the test data with many features:\n",
      "0.753461134895\n",
      "\n",
      "Ridge Regression R-squared for small model:\n",
      "0.730833757103\n",
      "\n",
      "Ridge Regression R-squared for large model:\n",
      "0.797642266828\n",
      "\n",
      "LASSO Regression R² for the test data with few features:\n",
      "0.722511709555\n",
      "\n",
      "LASSO Regression R² for the test data with many features:\n",
      "0.753461134895\n"
     ]
    }
   ],
   "source": [
    "## Final output\n",
    "## Run me last \n",
    "\n",
    "print('Logistic Regression R² for the test data with few features:')\n",
    "print(logisticRegFit.score(X_test, Y_test))\n",
    "print('\\nLogistic Regression R² for the test data with many features:')\n",
    "print(logisticRegBigFit.score(X_test2, Y_test))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Ridge Regression R-squared for small model:\")\n",
    "print(ridgeregr.score(X_test, Y_test))\n",
    "print(\"\")\n",
    "print(\"Ridge Regression R-squared for large model:\")\n",
    "print(ridgeregrBig.score(X_test2, Y_test))\n",
    "\n",
    "print(\"\")\n",
    "print('LASSO Regression R² for the test data with few features:')\n",
    "print(lass.score(X_test, Y_test))\n",
    "print('\\nLASSO Regression R² for the test data with many features:')\n",
    "print(lassBig.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"~/Documents/cancer-wisc2.csv\", header=None, dtype=\"int\")\n",
    "\n",
    "dataframe.columns = ['id', 'clump_thickness', 'cell_size', 'cell_shape', 'marginal_adhesion', 'epithelial_size', 'bare_nuclei', 'bland_chromatin', 'normal_nucleoli','mitoses', 'class']\n",
    "\n",
    "dataframe = dataframe.replace(to_replace=-1, value=np.nan)\n",
    "\n",
    "dataframe = dataframe.drop('id', 1)\n",
    "\n",
    "dataframe.dropna()\n",
    "\n",
    "dataframe.head()\n",
    "\n",
    "names = dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe.clump_thickness = dataframe.clump_thickness.apply(lambda x: float(x))\n",
    "dataframe.cell_size = dataframe.cell_size.apply(lambda x: float(x))\n",
    "dataframe.cell_shape = dataframe.cell_shape.apply(lambda x: float(x))\n",
    "dataframe.marginal_adhesion = dataframe.marginal_adhesion.apply(lambda x: float(x))\n",
    "dataframe.epithelial_size = dataframe.epithelial_size.apply(lambda x: float (x))\n",
    "dataframe.bare_nuclei = dataframe.bare_nuclei.apply(lambda x: float(x))\n",
    "dataframe.bland_chromatin = dataframe.bland_chromatin.apply(lambda x: float(x))\n",
    "dataframe.normal_nucleoli = dataframe.normal_nucleoli.apply(lambda x: float(x))\n",
    "dataframe.mitoses = dataframe.mitoses.apply(lambda x: float(x))\n",
    "dataframe['class'] = dataframe['class'].apply(lambda x: float(x))\n",
    "dataframe = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataframe[['clump_thickness', 'cell_size', 'cell_shape', \n",
    "               'marginal_adhesion', 'epithelial_size', 'bare_nuclei', \n",
    "               'bland_chromatin', 'normal_nucleoli','mitoses']]\n",
    "Y = dataframe['class']\n",
    "\n",
    "Y = Y.apply(lambda x: 1 if x > 3 else 0)\n",
    "dataframe['class'] = dataframe['class'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>cell_size</th>\n",
       "      <th>cell_shape</th>\n",
       "      <th>marginal_adhesion</th>\n",
       "      <th>epithelial_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chromatin</th>\n",
       "      <th>normal_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clump_thickness  cell_size  cell_shape  marginal_adhesion  epithelial_size  \\\n",
       "0            5.000      1.000       1.000              1.000            2.000   \n",
       "1            5.000      4.000       4.000              5.000            7.000   \n",
       "2            3.000      1.000       1.000              1.000            2.000   \n",
       "3            6.000      8.000       8.000              1.000            3.000   \n",
       "4            4.000      1.000       1.000              3.000            2.000   \n",
       "5            8.000     10.000      10.000              8.000            7.000   \n",
       "6            1.000      1.000       1.000              1.000            2.000   \n",
       "7            2.000      1.000       2.000              1.000            2.000   \n",
       "8            2.000      1.000       1.000              1.000            2.000   \n",
       "9            4.000      2.000       1.000              1.000            2.000   \n",
       "\n",
       "   bare_nuclei  bland_chromatin  normal_nucleoli  mitoses  class  \n",
       "0        1.000            3.000            1.000    1.000      0  \n",
       "1       10.000            3.000            2.000    1.000      0  \n",
       "2        2.000            3.000            1.000    1.000      0  \n",
       "3        4.000            3.000            7.000    1.000      0  \n",
       "4        1.000            3.000            1.000    1.000      0  \n",
       "5       10.000            9.000            7.000    1.000      1  \n",
       "6       10.000            3.000            1.000    1.000      0  \n",
       "7        1.000            3.000            1.000    1.000      0  \n",
       "8        1.000            1.000            1.000    5.000      0  \n",
       "9        1.000            2.000            1.000    1.000      0  "
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.075321\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  class   No. Observations:                  683\n",
      "Model:                          Logit   Df Residuals:                      673\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Tue, 09 May 2017   Pseudo R-squ.:                  0.8837\n",
      "Time:                        18:24:09   Log-Likelihood:                -51.444\n",
      "converged:                       True   LL-Null:                       -442.18\n",
      "                                        LLR p-value:                2.077e-162\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "clump_thickness       0.5350      0.142      3.767      0.000       0.257       0.813\n",
      "cell_size            -0.0063      0.209     -0.030      0.976      -0.416       0.404\n",
      "cell_shape            0.3227      0.231      1.399      0.162      -0.129       0.775\n",
      "marginal_adhesion     0.3306      0.123      2.678      0.007       0.089       0.573\n",
      "epithelial_size       0.0966      0.157      0.617      0.537      -0.210       0.404\n",
      "bare_nuclei           0.3830      0.094      4.082      0.000       0.199       0.567\n",
      "bland_chromatin       0.4472      0.171      2.609      0.009       0.111       0.783\n",
      "normal_nucleoli       0.2130      0.113      1.887      0.059      -0.008       0.434\n",
      "mitoses               0.5348      0.329      1.627      0.104      -0.110       1.179\n",
      "intercept           -10.1039      1.175     -8.600      0.000     -12.407      -7.801\n",
      "=====================================================================================\n",
      "\n",
      " Accuracy by Stage 4 cancer status\n",
      "col_0    0    1\n",
      "class          \n",
      "0      434   10\n",
      "1       11  228\n",
      "\n",
      " Percentage accuracy\n",
      "0.96925329429\n"
     ]
    }
   ],
   "source": [
    "# Declare predictors.\n",
    "X_statsmod = X\n",
    "\n",
    "# The Statsmodels formulation requires a column with constant value 1 that\n",
    "# will act as the intercept.\n",
    "X_statsmod['intercept'] = 1 \n",
    "\n",
    "# Declare and fit the model.\n",
    "logit = sm.Logit(Y, X_statsmod)\n",
    "result = logit.fit()\n",
    "\n",
    "# Lots of information about the model and its coefficients, but the\n",
    "# accuracy rate for predictions is missing.\n",
    "print(result.summary())\n",
    "\n",
    "# Calculate accuracy. First, get probability that each row will be admitted.\n",
    "pred_statsmod = result.predict(X_statsmod)\n",
    "\n",
    "# Code admission as 1 if probability is greater than .5.\n",
    "pred_y_statsmod = np.where(pred_statsmod < .5, 0, 1)\n",
    "\n",
    "# Accuracy table.\n",
    "table = pd.crosstab(Y, pred_y_statsmod)\n",
    "\n",
    "print('\\n Accuracy by Stage 4 cancer status')\n",
    "print(table)\n",
    "print('\\n Percentage accuracy')\n",
    "print((table.iloc[0,0] + table.iloc[1,1]) / (table.sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared simple model (TRAIN data):\n",
      "0.808576784249\n",
      "\n",
      "R-squared complex model (TRAIN data):\n",
      "0.845728517278\n",
      "\n",
      "Parameter Estimates for the same predictors for the small model and large model (TRAIN data):\n",
      "[[0.042 0.029]\n",
      " [0.005 0.086]\n",
      " [0.023 0.087]\n",
      " [0.016 0.015]\n",
      " [0.015 0.012]\n",
      " [0.044 0.081]\n",
      " [0.007 0.020]\n",
      " [0.015 0.005]\n",
      " [0.003 -0.015]\n",
      " [-0.225 -0.400]]\n",
      "\n",
      "\n",
      "R-squared simple model (test data):\n",
      "0.845511518194\n",
      "\n",
      "R-squared complex model (test data):\n",
      "0.867105297604\n"
     ]
    }
   ],
   "source": [
    "# Define the training and test sizes.\n",
    "trainsize = int(dataframe.shape[0] / 2)\n",
    "df_test = dataframe.iloc[trainsize:, :].copy()\n",
    "df_train = dataframe.iloc[:trainsize, :].copy()\n",
    "\n",
    "# Set up the regression model to predict solar radiation using all other\n",
    "# variables as features.\n",
    "regr1 = linear_model.LinearRegression()\n",
    "Y_train = df_train['class'].values.reshape(-1, 1)\n",
    "X_train = df_train.loc[:, ~(df_train.columns).isin(['class'])]\n",
    "regr1.fit(X_train, Y_train)\n",
    "print('\\nR-squared simple model (TRAIN data):')\n",
    "print(regr1.score(X_train, Y_train))\n",
    "\n",
    "#Store the parameter estimates.\n",
    "origparams = np.append(regr1.coef_, regr1.intercept_)\n",
    "\n",
    "# Make new features to capture potential quadratic and cubic relationships\n",
    "df_train['cell_size_by_bland_chromatin'] = df_train['cell_size'] * df_train['bland_chromatin']\n",
    "df_train['cell_shape_by_cell_size'] = df_train['cell_shape'] * df_train['cell_size']\n",
    "df_train['cell_shape_by_bland_chromatin'] = df_train['cell_shape'] * df_train['bland_chromatin']\n",
    "df_train['bare_nuclei^2'] = df_train['bare_nuclei'] ** 2\n",
    "df_train['mitoses^2'] = df_train['mitoses'] ** 2\n",
    "df_train['mitoses^3'] = df_train['mitoses'] ** 3\n",
    "\n",
    "# Re-run the model with the new features.\n",
    "regrBig = linear_model.LinearRegression()\n",
    "X_train2 = df_train.loc[:, ~(df_train.columns).isin(['class'])]\n",
    "regrBig.fit(X_train2, Y_train)\n",
    "print('\\nR-squared complex model (TRAIN data):')\n",
    "print(regrBig.score(X_train2, Y_train))\n",
    "\n",
    "# Store the new parameter estimates for the same features.\n",
    "newparams = np.append(\n",
    "    regrBig.coef_[0,0:(len(origparams)-1)],\n",
    "    regrBig.intercept_)\n",
    "\n",
    "print('\\nParameter Estimates for the same predictors for the small model '\n",
    "      'and large model (TRAIN data):')\n",
    "compare = np.column_stack((origparams, newparams))\n",
    "prettycompare = np.array2string(\n",
    "    compare,\n",
    "    formatter={'float_kind':'{0:.3f}'.format})\n",
    "print(prettycompare)\n",
    "print(\"\")\n",
    "\n",
    "# Test the simpler model with smaller coefficients.\n",
    "Y_test = df_test['class'].values.reshape(-1, 1)\n",
    "X_test = df_test.loc[:, ~(df_test.columns).isin(['class'])]\n",
    "print('\\nR-squared simple model (test data):')\n",
    "print(regr1.score(X_test, Y_test))\n",
    "\n",
    "# Make new features to capture potential quadratic and cubic relationships\n",
    "df_test['cell_size_by_bland_chromatin'] = df_test['cell_size'] * df_test['bland_chromatin']\n",
    "df_test['cell_shape_by_cell_size'] = df_test['cell_shape'] * df_test['cell_size']\n",
    "df_test['cell_shape_by_bland_chromatin'] = df_test['cell_shape'] * df_test['bland_chromatin']\n",
    "df_test['bare_nuclei^2'] = df_test['bare_nuclei'] ** 2\n",
    "df_test['mitoses^2'] = df_test['mitoses'] ** 2\n",
    "df_test['mitoses^3'] = df_test['mitoses'] ** 3\n",
    "\n",
    "# Re-run the model with the new features.\n",
    "X_test2 = df_test.loc[:, ~(df_test.columns).isin(['class'])]\n",
    "print('\\nR-squared complex model (test data):')\n",
    "print(regrBig.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Regression simple model (TRAIN data):\n",
      "0.772291783965\n",
      "\n",
      "[ 0.026574    0.0213824   0.02412437  0.01538304 -0.00288774  0.0466791\n",
      " -0.02274711  0.01858793 -0.00232403]\n",
      "\n",
      "\n",
      "Ridge Regression complex model (TRAIN data):\n",
      "0.823065069254\n",
      "\n",
      "Parameter Estimates for the same predictors for the small model and large model:\n",
      "[[0.027 0.030]\n",
      " [0.021 0.072]\n",
      " [0.024 0.073]\n",
      " [0.015 0.012]\n",
      " [-0.003 0.005]\n",
      " [0.047 0.037]\n",
      " [-0.023 -0.028]\n",
      " [0.019 0.013]\n",
      " [-0.002 -0.219]]\n",
      "\n",
      "Ridge Regression R-squared for small model:\n",
      "0.730833757103\n",
      "\n",
      "Ridge Regression R-squared for large model:\n",
      "0.797642266828\n"
     ]
    }
   ],
   "source": [
    "# Fitting a ridge regression model. Alpha(lambda) is the regularization\n",
    "# parameter (usually called lambda). As alpha gets larger, parameter\n",
    "# shrinkage grows more pronounced. Note that by convention, the\n",
    "# intercept is not regularized. Since we standardized the data\n",
    "# earlier, the intercept should be equal to zero and can be dropped.\n",
    "\n",
    "ridgeregr = linear_model.Ridge(alpha=12, fit_intercept=False, normalize=False) \n",
    "ridgeregr.fit(X_train, Y_train)\n",
    "print('\\nRidge Regression simple model (TRAIN data):')\n",
    "\n",
    "print(ridgeregr.score(X_train, Y_train))\n",
    "print(\"\")\n",
    "origparams = ridgeregr.coef_[0] #leaving out intercept\n",
    "print(origparams)\n",
    "\n",
    "ridgeregrBig = linear_model.Ridge(alpha=12, fit_intercept=False, normalize=True)\n",
    "ridgeregrBig.fit(X_train2, Y_train)\n",
    "print(\"\")\n",
    "print('\\nRidge Regression complex model (TRAIN data):')\n",
    "print(ridgeregrBig.score(X_train2, Y_train))\n",
    "newparams = ridgeregrBig.coef_[0, 0:len(origparams)] #leaving out intercept\n",
    "\n",
    "print('\\nParameter Estimates for the same predictors for the small model'\n",
    "      ' and large model:')\n",
    "compare = np.column_stack((origparams, newparams))\n",
    "prettycompare = np.array2string(\n",
    "    compare,\n",
    "    formatter={'float_kind':'{0:.3f}'.format})\n",
    "print(prettycompare)\n",
    "print(\"\")\n",
    "print(\"Ridge Regression R-squared for small model:\")\n",
    "print(ridgeregr.score(X_test, Y_test))\n",
    "print(\"\")\n",
    "print(\"Ridge Regression R-squared for large model:\")\n",
    "print(ridgeregrBig.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for the model with few features:\n",
      "0.733213555375\n",
      "\n",
      "Parameter estimates for the model with few features:\n",
      "[ 0.02343462  0.00957195  0.01784776  0.          0.          0.05013214\n",
      "  0.          0.00674197  0.          0.00895562]\n",
      "\n",
      "R² for the model with many features:\n",
      "0.679890180491\n",
      "\n",
      "Parameter estimates for the model with many features:\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.00135676  0.00268882  0.00297218  0.00612355\n",
      "  0.          0.00027119  0.10390464]\n",
      "\n",
      "R² for the test data with few features:\n",
      "0.722511709555\n",
      "\n",
      "R² for the test data with many features:\n",
      "0.753461134895\n"
     ]
    }
   ],
   "source": [
    "# LASSO Regression Model\n",
    "\n",
    "# Small number of parameters.\n",
    "lass = linear_model.Lasso(alpha=.35, normalize=False)\n",
    "lassfit = lass.fit(X_train, Y_train)\n",
    "print('R² for the model with few features:')\n",
    "print(lass.score(X_train, Y_train))\n",
    "origparams = np.append(lassfit.coef_, lassfit.intercept_)\n",
    "print('\\nParameter estimates for the model with few features:')\n",
    "print(origparams)\n",
    "\n",
    "# Large number of parameters.\n",
    "lassBig = linear_model.Lasso(alpha=.35, normalize=False)\n",
    "lassBig.fit(X_train2, Y_train)\n",
    "print('\\nR² for the model with many features:')\n",
    "print(lassBig.score(X_train2, Y_train))\n",
    "origparams = np.append(lassBig.coef_, lassBig.intercept_)\n",
    "print('\\nParameter estimates for the model with many features:')\n",
    "print(origparams)\n",
    "\n",
    "print(\"\")\n",
    "print('R² for the test data with few features:')\n",
    "print(lass.score(X_test, Y_test))\n",
    "print('\\nR² for the test data with many features:')\n",
    "print(lassBig.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: R² for the TRAIN model with few features:\n",
      "0.958944281525\n",
      "\n",
      "Logistic Regression: Parameter estimates for the model with few features:\n",
      "[  2.73029848e-01   7.07713870e-02   3.08744365e-01   2.61712642e-01\n",
      "   1.29082722e-03   3.25903482e-01  -6.89581200e-02   1.09527075e-01\n",
      "   1.48736229e-01  -4.94586128e+00]\n",
      "\n",
      "R² for the TRAIN model with many features:\n",
      "0.679890180491\n",
      "\n",
      "Parameter estimates for the model with many features:\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.00135676  0.00268882  0.00297218  0.00612355\n",
      "  0.          0.00027119  0.10390464]\n",
      "\n",
      "R² for the test data with few features:\n",
      "0.985380116959\n",
      "\n",
      "R² for the test data with many features:\n",
      "0.753461134895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "logisticReg = linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n",
    "    C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "    random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "    verbose=0, warm_start=False, n_jobs=1)\n",
    "logisticRegFit = logisticReg.fit(X_train, Y_train)\n",
    "print('Logistic Regression: R² for the TRAIN model with few features:')\n",
    "print(logisticRegFit.score(X_train, Y_train))\n",
    "origparams = np.append(logisticRegFit.coef_, logisticRegFit.intercept_)\n",
    "print('\\nLogistic Regression: Parameter estimates for the model with few features:')\n",
    "print(origparams)\n",
    "\n",
    "# Large number of parameters.\n",
    "logisticRegBig = linear_model.Lasso(alpha=.35, normalize=False)\n",
    "logisticRegBigFit = logisticRegBig.fit(X_train2, Y_train)\n",
    "print('\\nR² for the TRAIN model with many features:')\n",
    "print(logisticRegBigFit.score(X_train2, Y_train))\n",
    "origparams = np.append(logisticRegBigFit.coef_, logisticRegBigFit.intercept_)\n",
    "print('\\nParameter estimates for the model with many features:')\n",
    "print(origparams)\n",
    "\n",
    "print(\"\")\n",
    "print('R² for the test data with few features:')\n",
    "print(logisticRegFit.score(X_test, Y_test))\n",
    "print('\\nR² for the test data with many features:')\n",
    "print(logisticRegBigFit.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
