{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4.3.6: Make Your Network  \n",
    "## Kevin Hahn  \n",
    "\n",
    "After some attempts to practice multi-layer perceptron methods on large datasets, I decided to apply my learning on a much smaller dataset, Gorman and Sejnowski's Sonar dataset which classifies sonar signlas bounced off a metal cylinder and those bounced off an approximately cylindrial rock. This dataset has 208 observations with 60 different frequency chirps and 1 label as either Rock or Mine.  \n",
    "\n",
    "I built a MLP Classifier from scratch with the help of a guide as well as through the MLPClassifier method in SKLearn. With a three-layered perceptron consisting of 1000, 30, and 10 nodes each, the MLP Classifier model found an accuracy rating of 0.8702. Testing this model using a training set (80%) to predict values of a validation set (20%) similarly found an accuracy rating of 0.8810. However, when changing the random_state parameter of the MLP Classifier, the performance of the model seemed to vary considerably.\n",
    "\n",
    "From there I decided to build and iterate upon supervised learning and boosting models as a comparison. After ensembling several methods together, I decided to keep the Decision Tree, Random Forest, Ada Boosting, Gradient Boosting, Extra Trees, and Multi-Layer Perceptron Classifier models and run each with 10 KFolds. Tuning the hyperparameters Decision Tree and Random Forest models helped decrease the variability of the accuracy ratings they each reported, and I threw in the less familiar Ada Boosting and Gradient Boosting methods just for fun.  \n",
    "\n",
    "Interestingly, the computational complexity of the Random Forest Classifier seemed to be the highest and took the most time. I suspect that if this were a much larger dataset (e.g., far more observation rows), MLP would take even longer than Random Forest.  \n",
    "\n",
    "While the MLP Classifier did have the one highest accuracy rating, the method's variability had the largest range compared to the other methods. Random Forest, however, had the least amount of variability and similar median accuracy rating to MLP. AdaBoost had the highest median accuracy and similar variability in its accruacy ratings when compared to Decision Tree, Gradient Boosting, and Extra Trees Classifier methods.  \n",
    "\n",
    "The next two cells in this notebook consist of the deliverables I'm proudest of for this challenge: first, the boxplot of each of the models' average performance when implemented 10 times each, and second, the conditional logic I used to test for accuracy of the MLP built by the training data on a separate hold-out validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAJMCAYAAACPXIWpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKIRAAwAAKIRAAwAAKIRAAwAAKMT/B5jQGVndeJlyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12cba30f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Figure 1\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"Average Performance of Each Model with 10 KFolds\")\n",
    "sns.set_style(style=\"whitegrid\")\n",
    "initial = sns.boxplot(data=results, palette='Set1', notch=False)\n",
    "initial.set_xticklabels(['DTC','RFC','AB','GBM','ET','MLP'])\n",
    "initial.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8809523809523809\n"
     ]
    }
   ],
   "source": [
    "### Testing Training vs. Validation sets in predicting accuracy\n",
    "\n",
    "#checking accuracy of mlp on predicting correct Rock or Mine\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000, 30, 10),\n",
    "                    tol=0.001,\n",
    "                    learning_rate_init=0.001,                    \n",
    "                    random_state=99)\n",
    "mlp.fit(X_train, Y_train)\n",
    "\n",
    "#using the training set to model to predict on validation set\n",
    "accuracy = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "\n",
    "for row in X_validation:\n",
    "    guess = mlp.predict(row.reshape(1,-1))\n",
    "    if guess[0] == Y_validation[i]:\n",
    "        correct += 1\n",
    "    i += 1\n",
    "    total += 1\n",
    "accuracy = correct / total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing required packages\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"~/Documents/sonar.all-data\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# Y = Y.apply(lambda x: 1 if x == 'M' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.3039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.1321</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.1710</td>\n",
       "      <td>0.0731</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.3513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.1158</td>\n",
       "      <td>0.0922</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0671</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.1122</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.1009</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.1197</td>\n",
       "      <td>0.1589</td>\n",
       "      <td>0.1392</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>0.0955</td>\n",
       "      <td>0.1895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.0597</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.0531</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.1052</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0615</td>\n",
       "      <td>0.0650</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.1615</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.2176</td>\n",
       "      <td>0.2033</td>\n",
       "      <td>0.1459</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1683</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.2912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0774</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>0.1026</td>\n",
       "      <td>0.1138</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.1520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.2559</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>0.4110</td>\n",
       "      <td>0.4983</td>\n",
       "      <td>0.5920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0819</td>\n",
       "      <td>0.1252</td>\n",
       "      <td>0.1783</td>\n",
       "      <td>0.3070</td>\n",
       "      <td>0.3008</td>\n",
       "      <td>0.2362</td>\n",
       "      <td>0.3830</td>\n",
       "      <td>0.3759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.2353</td>\n",
       "      <td>0.1838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>0.1077</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>0.2385</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.1882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.1058</td>\n",
       "      <td>0.1023</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0644</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0816</td>\n",
       "      <td>0.0993</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0736</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.1097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0599</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.1163</td>\n",
       "      <td>0.1734</td>\n",
       "      <td>0.1679</td>\n",
       "      <td>0.1119</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0630</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.0688</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>0.0624</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.0651</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>0.2668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.1451</td>\n",
       "      <td>0.1789</td>\n",
       "      <td>0.2522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.0629</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.1695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.0551</td>\n",
       "      <td>0.0597</td>\n",
       "      <td>0.1416</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>0.1618</td>\n",
       "      <td>0.2558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.1827</td>\n",
       "      <td>0.1106</td>\n",
       "      <td>0.1702</td>\n",
       "      <td>0.2804</td>\n",
       "      <td>0.4432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.0709</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.0961</td>\n",
       "      <td>0.1323</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>0.2592</td>\n",
       "      <td>0.3745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>0.0682</td>\n",
       "      <td>0.0688</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>0.0955</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.2546</td>\n",
       "      <td>0.2952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.0707</td>\n",
       "      <td>0.1313</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.2524</td>\n",
       "      <td>0.3595</td>\n",
       "      <td>0.5915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0381</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.1850</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>0.4117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.0698</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1438</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0566</td>\n",
       "      <td>0.0759</td>\n",
       "      <td>0.0679</td>\n",
       "      <td>0.0970</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.2544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0845</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.1569</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.1463</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.1923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>0.0902</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.1533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.1146</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.1859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>0.0874</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.1747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.0491</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.2471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>0.0662</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.0879</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.1977</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.0366</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0766</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.1594</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.1069</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>0.2271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0721</td>\n",
       "      <td>0.1341</td>\n",
       "      <td>0.1626</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.3193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>0.0398</td>\n",
       "      <td>0.0570</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1091</td>\n",
       "      <td>0.1709</td>\n",
       "      <td>0.1684</td>\n",
       "      <td>0.1865</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0848</td>\n",
       "      <td>0.1127</td>\n",
       "      <td>0.1103</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.2337</td>\n",
       "      <td>0.3113</td>\n",
       "      <td>0.3997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows  61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "5    0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
       "6    0.0317  0.0956  0.1321  0.1408  0.1674  0.1710  0.0731  0.1401  0.2083   \n",
       "7    0.0519  0.0548  0.0842  0.0319  0.1158  0.0922  0.1027  0.0613  0.1465   \n",
       "8    0.0223  0.0375  0.0484  0.0475  0.0647  0.0591  0.0753  0.0098  0.0684   \n",
       "9    0.0164  0.0173  0.0347  0.0070  0.0187  0.0671  0.1056  0.0697  0.0962   \n",
       "10   0.0039  0.0063  0.0152  0.0336  0.0310  0.0284  0.0396  0.0272  0.0323   \n",
       "11   0.0123  0.0309  0.0169  0.0313  0.0358  0.0102  0.0182  0.0579  0.1122   \n",
       "12   0.0079  0.0086  0.0055  0.0250  0.0344  0.0546  0.0528  0.0958  0.1009   \n",
       "13   0.0090  0.0062  0.0253  0.0489  0.1197  0.1589  0.1392  0.0987  0.0955   \n",
       "14   0.0124  0.0433  0.0604  0.0449  0.0597  0.0355  0.0531  0.0343  0.1052   \n",
       "15   0.0298  0.0615  0.0650  0.0921  0.1615  0.2294  0.2176  0.2033  0.1459   \n",
       "16   0.0352  0.0116  0.0191  0.0469  0.0737  0.1185  0.1683  0.1541  0.1466   \n",
       "17   0.0192  0.0607  0.0378  0.0774  0.1388  0.0809  0.0568  0.0219  0.1037   \n",
       "18   0.0270  0.0092  0.0145  0.0278  0.0412  0.0757  0.1026  0.1138  0.0794   \n",
       "19   0.0126  0.0149  0.0641  0.1732  0.2565  0.2559  0.2947  0.4110  0.4983   \n",
       "20   0.0473  0.0509  0.0819  0.1252  0.1783  0.3070  0.3008  0.2362  0.3830   \n",
       "21   0.0664  0.0575  0.0842  0.0372  0.0458  0.0771  0.0771  0.1130  0.2353   \n",
       "22   0.0099  0.0484  0.0299  0.0297  0.0652  0.1077  0.2363  0.2385  0.0075   \n",
       "23   0.0115  0.0150  0.0136  0.0076  0.0211  0.1058  0.1023  0.0440  0.0931   \n",
       "24   0.0293  0.0644  0.0390  0.0173  0.0476  0.0816  0.0993  0.0315  0.0736   \n",
       "25   0.0201  0.0026  0.0138  0.0062  0.0133  0.0151  0.0541  0.0210  0.0505   \n",
       "26   0.0151  0.0320  0.0599  0.1050  0.1163  0.1734  0.1679  0.1119  0.0889   \n",
       "27   0.0177  0.0300  0.0288  0.0394  0.0630  0.0526  0.0688  0.0633  0.0624   \n",
       "28   0.0100  0.0275  0.0190  0.0371  0.0416  0.0201  0.0314  0.0651  0.1896   \n",
       "29   0.0189  0.0308  0.0197  0.0622  0.0080  0.0789  0.1440  0.1451  0.1789   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "178  0.0197  0.0394  0.0384  0.0076  0.0251  0.0629  0.0747  0.0578  0.1357   \n",
       "179  0.0394  0.0420  0.0446  0.0551  0.0597  0.1416  0.0956  0.0802  0.1618   \n",
       "180  0.0310  0.0221  0.0433  0.0191  0.0964  0.1827  0.1106  0.1702  0.2804   \n",
       "181  0.0423  0.0321  0.0709  0.0108  0.1070  0.0973  0.0961  0.1323  0.2462   \n",
       "182  0.0095  0.0308  0.0539  0.0411  0.0613  0.1039  0.1016  0.1394  0.2592   \n",
       "183  0.0096  0.0404  0.0682  0.0688  0.0887  0.0932  0.0955  0.2140  0.2546   \n",
       "184  0.0269  0.0383  0.0505  0.0707  0.1313  0.2103  0.2263  0.2524  0.3595   \n",
       "185  0.0340  0.0625  0.0381  0.0257  0.0441  0.1027  0.1287  0.1850  0.2647   \n",
       "186  0.0209  0.0191  0.0411  0.0321  0.0698  0.1579  0.1438  0.1402  0.3048   \n",
       "187  0.0368  0.0279  0.0103  0.0566  0.0759  0.0679  0.0970  0.1473  0.2164   \n",
       "188  0.0089  0.0274  0.0248  0.0237  0.0224  0.0845  0.1488  0.1224  0.1569   \n",
       "189  0.0158  0.0239  0.0150  0.0494  0.0988  0.1425  0.1463  0.1219  0.1697   \n",
       "190  0.0156  0.0210  0.0282  0.0596  0.0462  0.0779  0.1365  0.0780  0.1038   \n",
       "191  0.0315  0.0252  0.0167  0.0479  0.0902  0.1057  0.1024  0.1209  0.1241   \n",
       "192  0.0056  0.0267  0.0221  0.0561  0.0936  0.1146  0.0706  0.0996  0.1673   \n",
       "193  0.0203  0.0121  0.0380  0.0128  0.0537  0.0874  0.1021  0.0852  0.1136   \n",
       "194  0.0392  0.0108  0.0267  0.0257  0.0410  0.0491  0.1053  0.1690  0.2105   \n",
       "195  0.0129  0.0141  0.0309  0.0375  0.0767  0.0787  0.0662  0.1108  0.1777   \n",
       "196  0.0050  0.0017  0.0270  0.0450  0.0958  0.0830  0.0879  0.1220  0.1977   \n",
       "197  0.0366  0.0421  0.0504  0.0250  0.0596  0.0252  0.0958  0.0991  0.1419   \n",
       "198  0.0238  0.0318  0.0422  0.0399  0.0788  0.0766  0.0881  0.1143  0.1594   \n",
       "199  0.0116  0.0744  0.0367  0.0225  0.0076  0.0545  0.1110  0.1069  0.1708   \n",
       "200  0.0131  0.0387  0.0329  0.0078  0.0721  0.1341  0.1626  0.1902  0.2610   \n",
       "201  0.0335  0.0258  0.0398  0.0570  0.0529  0.1091  0.1709  0.1684  0.1865   \n",
       "202  0.0272  0.0378  0.0488  0.0848  0.1127  0.1103  0.1349  0.2337  0.3113   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9  ...      51      52      53      54      55      56      57  \\\n",
       "0    0.2111 ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1    0.2872 ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2    0.6194 ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3    0.1264 ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4    0.4459 ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "5    0.3039 ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027   \n",
       "6    0.3513 ...  0.0201  0.0248  0.0131  0.0070  0.0138  0.0092  0.0143   \n",
       "7    0.2838 ...  0.0081  0.0120  0.0045  0.0121  0.0097  0.0085  0.0047   \n",
       "8    0.1487 ...  0.0145  0.0128  0.0145  0.0058  0.0049  0.0065  0.0093   \n",
       "9    0.0251 ...  0.0090  0.0223  0.0179  0.0084  0.0068  0.0032  0.0035   \n",
       "10   0.0452 ...  0.0062  0.0120  0.0052  0.0056  0.0093  0.0042  0.0003   \n",
       "11   0.0835 ...  0.0133  0.0265  0.0224  0.0074  0.0118  0.0026  0.0092   \n",
       "12   0.1240 ...  0.0176  0.0127  0.0088  0.0098  0.0019  0.0059  0.0058   \n",
       "13   0.1895 ...  0.0059  0.0095  0.0194  0.0080  0.0152  0.0158  0.0053   \n",
       "14   0.2120 ...  0.0083  0.0057  0.0174  0.0188  0.0054  0.0114  0.0196   \n",
       "15   0.0852 ...  0.0031  0.0153  0.0071  0.0212  0.0076  0.0152  0.0049   \n",
       "16   0.2912 ...  0.0346  0.0158  0.0154  0.0109  0.0048  0.0095  0.0015   \n",
       "17   0.1186 ...  0.0331  0.0131  0.0120  0.0108  0.0024  0.0045  0.0037   \n",
       "18   0.1520 ...  0.0084  0.0010  0.0018  0.0068  0.0039  0.0120  0.0132   \n",
       "19   0.5920 ...  0.0092  0.0035  0.0098  0.0121  0.0006  0.0181  0.0094   \n",
       "20   0.3759 ...  0.0193  0.0118  0.0064  0.0042  0.0054  0.0049  0.0082   \n",
       "21   0.1838 ...  0.0141  0.0190  0.0043  0.0036  0.0026  0.0024  0.0162   \n",
       "22   0.1882 ...  0.0173  0.0149  0.0115  0.0202  0.0139  0.0029  0.0160   \n",
       "23   0.0734 ...  0.0091  0.0016  0.0084  0.0064  0.0026  0.0029  0.0037   \n",
       "24   0.0860 ...  0.0035  0.0052  0.0083  0.0078  0.0075  0.0105  0.0160   \n",
       "25   0.1097 ...  0.0108  0.0070  0.0063  0.0030  0.0011  0.0007  0.0024   \n",
       "26   0.1205 ...  0.0061  0.0015  0.0084  0.0128  0.0054  0.0011  0.0019   \n",
       "27   0.0613 ...  0.0102  0.0122  0.0044  0.0075  0.0124  0.0099  0.0057   \n",
       "28   0.2668 ...  0.0088  0.0104  0.0036  0.0088  0.0047  0.0117  0.0020   \n",
       "29   0.2522 ...  0.0038  0.0096  0.0142  0.0190  0.0140  0.0099  0.0092   \n",
       "..      ... ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "178  0.1695 ...  0.0134  0.0097  0.0042  0.0058  0.0072  0.0041  0.0045   \n",
       "179  0.2558 ...  0.0146  0.0040  0.0114  0.0032  0.0062  0.0101  0.0068   \n",
       "180  0.4432 ...  0.0204  0.0059  0.0053  0.0079  0.0037  0.0015  0.0056   \n",
       "181  0.2696 ...  0.0176  0.0035  0.0093  0.0121  0.0075  0.0056  0.0021   \n",
       "182  0.3745 ...  0.0181  0.0019  0.0102  0.0133  0.0040  0.0042  0.0030   \n",
       "183  0.2952 ...  0.0237  0.0078  0.0144  0.0170  0.0012  0.0109  0.0036   \n",
       "184  0.5915 ...  0.0167  0.0199  0.0145  0.0081  0.0045  0.0043  0.0027   \n",
       "185  0.4117 ...  0.0141  0.0019  0.0067  0.0099  0.0042  0.0057  0.0051   \n",
       "186  0.3914 ...  0.0078  0.0201  0.0104  0.0039  0.0031  0.0062  0.0087   \n",
       "187  0.2544 ...  0.0105  0.0024  0.0018  0.0057  0.0092  0.0009  0.0086   \n",
       "188  0.2119 ...  0.0096  0.0103  0.0093  0.0025  0.0044  0.0021  0.0069   \n",
       "189  0.1923 ...  0.0121  0.0108  0.0057  0.0028  0.0079  0.0034  0.0046   \n",
       "190  0.1567 ...  0.0150  0.0060  0.0082  0.0091  0.0038  0.0056  0.0056   \n",
       "191  0.1533 ...  0.0108  0.0062  0.0044  0.0072  0.0007  0.0054  0.0035   \n",
       "192  0.1859 ...  0.0072  0.0055  0.0074  0.0068  0.0084  0.0037  0.0024   \n",
       "193  0.1747 ...  0.0134  0.0094  0.0047  0.0045  0.0042  0.0028  0.0036   \n",
       "194  0.2471 ...  0.0083  0.0080  0.0026  0.0079  0.0042  0.0071  0.0044   \n",
       "195  0.2245 ...  0.0124  0.0093  0.0072  0.0019  0.0027  0.0054  0.0017   \n",
       "196  0.2282 ...  0.0165  0.0056  0.0010  0.0027  0.0062  0.0024  0.0063   \n",
       "197  0.1847 ...  0.0132  0.0027  0.0022  0.0059  0.0016  0.0025  0.0017   \n",
       "198  0.2048 ...  0.0096  0.0071  0.0084  0.0038  0.0026  0.0028  0.0013   \n",
       "199  0.2271 ...  0.0141  0.0103  0.0100  0.0034  0.0026  0.0037  0.0044   \n",
       "200  0.3193 ...  0.0150  0.0076  0.0032  0.0037  0.0071  0.0040  0.0009   \n",
       "201  0.2660 ...  0.0120  0.0039  0.0053  0.0062  0.0046  0.0045  0.0022   \n",
       "202  0.3997 ...  0.0091  0.0045  0.0043  0.0043  0.0098  0.0054  0.0051   \n",
       "203  0.2684 ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
       "204  0.2154 ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
       "205  0.2529 ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "206  0.2354 ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
       "207  0.2354 ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
       "\n",
       "         58      59  60  \n",
       "0    0.0090  0.0032   R  \n",
       "1    0.0052  0.0044   R  \n",
       "2    0.0095  0.0078   R  \n",
       "3    0.0040  0.0117   R  \n",
       "4    0.0107  0.0094   R  \n",
       "5    0.0051  0.0062   R  \n",
       "6    0.0036  0.0103   R  \n",
       "7    0.0048  0.0053   R  \n",
       "8    0.0059  0.0022   R  \n",
       "9    0.0056  0.0040   R  \n",
       "10   0.0053  0.0036   R  \n",
       "11   0.0009  0.0044   R  \n",
       "12   0.0059  0.0032   R  \n",
       "13   0.0189  0.0102   R  \n",
       "14   0.0147  0.0062   R  \n",
       "15   0.0200  0.0073   R  \n",
       "16   0.0073  0.0067   R  \n",
       "17   0.0112  0.0075   R  \n",
       "18   0.0070  0.0088   R  \n",
       "19   0.0116  0.0063   R  \n",
       "20   0.0028  0.0027   R  \n",
       "21   0.0109  0.0079   R  \n",
       "22   0.0106  0.0134   R  \n",
       "23   0.0070  0.0041   R  \n",
       "24   0.0095  0.0011   R  \n",
       "25   0.0057  0.0044   R  \n",
       "26   0.0023  0.0062   R  \n",
       "27   0.0032  0.0019   R  \n",
       "28   0.0091  0.0058   R  \n",
       "29   0.0052  0.0075   R  \n",
       "..      ...     ...  ..  \n",
       "178  0.0047  0.0054   M  \n",
       "179  0.0053  0.0087   M  \n",
       "180  0.0067  0.0054   M  \n",
       "181  0.0043  0.0017   M  \n",
       "182  0.0031  0.0033   M  \n",
       "183  0.0043  0.0018   M  \n",
       "184  0.0055  0.0057   M  \n",
       "185  0.0033  0.0058   M  \n",
       "186  0.0070  0.0042   M  \n",
       "187  0.0110  0.0052   M  \n",
       "188  0.0060  0.0018   M  \n",
       "189  0.0022  0.0021   M  \n",
       "190  0.0048  0.0024   M  \n",
       "191  0.0001  0.0055   M  \n",
       "192  0.0034  0.0007   M  \n",
       "193  0.0013  0.0016   M  \n",
       "194  0.0022  0.0014   M  \n",
       "195  0.0024  0.0029   M  \n",
       "196  0.0017  0.0028   M  \n",
       "197  0.0027  0.0027   M  \n",
       "198  0.0035  0.0060   M  \n",
       "199  0.0057  0.0035   M  \n",
       "200  0.0015  0.0085   M  \n",
       "201  0.0005  0.0031   M  \n",
       "202  0.0065  0.0103   M  \n",
       "203  0.0193  0.0157   M  \n",
       "204  0.0062  0.0067   M  \n",
       "205  0.0077  0.0031   M  \n",
       "206  0.0036  0.0048   M  \n",
       "207  0.0061  0.0115   M  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def create_baseline():\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 76.35% (8.12%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=create_baseline, nb_epoch=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 84.11% (4.36%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller: 85.06% (5.58%)\n"
     ]
    }
   ],
   "source": [
    "# smaller model\n",
    "def create_smaller():\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 85.52% (5.36%)\n"
     ]
    }
   ],
   "source": [
    "# larger model\n",
    "def create_larger():\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(1000, 30, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=99, shuffle=True,\n",
       "       solver='adam', tol=0.001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the model.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Establish and fit the model, with three layers of 1000, 30, and 10 nodes each.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000, 30, 10),\n",
    "                    tol=0.001,\n",
    "                    learning_rate_init=0.001,                    \n",
    "                    random_state=99)\n",
    "mlp.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87019230769230771"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(correct)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8809523809523809\n"
     ]
    }
   ],
   "source": [
    "#checking accuracy of mlp on predicting correct Rock or Mine\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000, 30, 10),\n",
    "                    tol=0.001,\n",
    "                    learning_rate_init=0.001,                    \n",
    "                    random_state=99)\n",
    "mlp.fit(X_train, Y_train)\n",
    "\n",
    "#using the training set to model to predict on validation set\n",
    "accuracy = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "\n",
    "for row in X_validation:\n",
    "    guess = mlp.predict(row.reshape(1,-1))\n",
    "    if guess[0] == Y_validation[i]:\n",
    "        correct += 1\n",
    "    i += 1\n",
    "    total += 1\n",
    "accuracy = correct / total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02  ,  0.0371,  0.0428,  0.0207,  0.0954,  0.0986,  0.1539,\n",
       "        0.1601,  0.3109,  0.2111,  0.1609,  0.1582,  0.2238,  0.0645,\n",
       "        0.066 ,  0.2273,  0.31  ,  0.2999,  0.5078,  0.4797,  0.5783,\n",
       "        0.5071,  0.4328,  0.555 ,  0.6711,  0.6415,  0.7104,  0.808 ,\n",
       "        0.6791,  0.3857,  0.1307,  0.2604,  0.5121,  0.7547,  0.8537,\n",
       "        0.8507,  0.6692,  0.6097,  0.4943,  0.2744,  0.051 ,  0.2834,\n",
       "        0.2825,  0.4256,  0.2641,  0.1386,  0.1051,  0.1343,  0.0383,\n",
       "        0.0324,  0.0232,  0.0027,  0.0065,  0.0159,  0.0072,  0.0167,\n",
       "        0.018 ,  0.0084,  0.009 ,  0.0032])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(60, 30, 5), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=99, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(60, 30, 5),\n",
    "                   random_state=99)\n",
    "mlp.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79807692307692313"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9  ...      51      52      53      54      55      56      57      58  \\\n",
       "0  0.2111 ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084  0.0090   \n",
       "1  0.2872 ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049  0.0052   \n",
       "2  0.6194 ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164  0.0095   \n",
       "3  0.1264 ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044  0.0040   \n",
       "4  0.4459 ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048  0.0107   \n",
       "\n",
       "       59  60  \n",
       "0  0.0032   R  \n",
       "1  0.0044   R  \n",
       "2  0.0078   R  \n",
       "3  0.0117   R  \n",
       "4  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Import All Required Packages\n",
    "\n",
    "#The Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#Decision Tree Classifier\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "#Random Forest Classifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "#Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#SVM Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Compare Algorithms\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot # delete??\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#AdaBoost Classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#Stochastic Gradient Boosting Classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Voting Ensemble for Classification\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Grid Search for Algorithm Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Randomized for Algorithm Tuning\n",
    "import numpy\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Confusion Matrix and Accuracy Report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC: 0.620714 (0.137775)\n",
      "RFC: 0.558333 (0.114421)\n",
      "AB: 0.684286 (0.147754)\n",
      "GBM: 0.568810 (0.144688)\n",
      "ET: 0.573333 (0.174164)\n",
      "MLP: 0.589048 (0.239943)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAJMCAYAAACPXIWpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCDQAAIBC/H/YSxyZeGOhuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x131d1b1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "X = X\n",
    "Y = Y\n",
    "\n",
    "# prepare models    --- pared down to supervised methods that seem to work better, boosting models, and MLP\n",
    "models = []\n",
    "models.append(('DTC', DecisionTreeClassifier(criterion=\"entropy\",\n",
    "                                             max_features='auto',\n",
    "                                             max_depth=None,\n",
    "                                             random_state=99\n",
    "                                        )))\n",
    "models.append(('RFC', RandomForestClassifier(n_estimators=1000,\n",
    "    criterion='entropy',\n",
    "    max_features='auto',\n",
    "    max_depth=None,\n",
    "    random_state=99)))\n",
    "models.append(('AB', AdaBoostClassifier(random_state=99))) \n",
    "models.append(('GBM', GradientBoostingClassifier(random_state=99))) \n",
    "models.append(('ET', ExtraTreesClassifier(random_state=99))) \n",
    "models.append(('MLP', MLPClassifier(hidden_layer_sizes=(1000, 30, 10),\n",
    "                    tol=0.001,\n",
    "                    learning_rate_init=0.001,                    \n",
    "                    random_state=99)))\n",
    "\n",
    "# evaluate each type of classifier model\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "  kfold = KFold(n_splits=10, random_state=27)\n",
    "  cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"Average Performance of Each Model with 10 KFolds\")\n",
    "sns.set_style(style=\"whitegrid\")\n",
    "initial = sns.boxplot(data=results, palette='Set1', notch=False)\n",
    "initial.set_xticklabels(['DTC','RFC','AB','GBM','ET','MLP'])\n",
    "initial.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.3039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.1321</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.1710</td>\n",
       "      <td>0.0731</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.3513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.1158</td>\n",
       "      <td>0.0922</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0671</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.1122</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows  61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3       4       5       6       7       8   \\\n",
       "0   0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1   0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2   0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3   0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4   0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "5   0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
       "6   0.0317  0.0956  0.1321  0.1408  0.1674  0.1710  0.0731  0.1401  0.2083   \n",
       "7   0.0519  0.0548  0.0842  0.0319  0.1158  0.0922  0.1027  0.0613  0.1465   \n",
       "8   0.0223  0.0375  0.0484  0.0475  0.0647  0.0591  0.0753  0.0098  0.0684   \n",
       "9   0.0164  0.0173  0.0347  0.0070  0.0187  0.0671  0.1056  0.0697  0.0962   \n",
       "10  0.0039  0.0063  0.0152  0.0336  0.0310  0.0284  0.0396  0.0272  0.0323   \n",
       "11  0.0123  0.0309  0.0169  0.0313  0.0358  0.0102  0.0182  0.0579  0.1122   \n",
       "\n",
       "        9  ...      51      52      53      54      55      56      57  \\\n",
       "0   0.2111 ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1   0.2872 ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2   0.6194 ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3   0.1264 ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4   0.4459 ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "5   0.3039 ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027   \n",
       "6   0.3513 ...  0.0201  0.0248  0.0131  0.0070  0.0138  0.0092  0.0143   \n",
       "7   0.2838 ...  0.0081  0.0120  0.0045  0.0121  0.0097  0.0085  0.0047   \n",
       "8   0.1487 ...  0.0145  0.0128  0.0145  0.0058  0.0049  0.0065  0.0093   \n",
       "9   0.0251 ...  0.0090  0.0223  0.0179  0.0084  0.0068  0.0032  0.0035   \n",
       "10  0.0452 ...  0.0062  0.0120  0.0052  0.0056  0.0093  0.0042  0.0003   \n",
       "11  0.0835 ...  0.0133  0.0265  0.0224  0.0074  0.0118  0.0026  0.0092   \n",
       "\n",
       "        58      59  60  \n",
       "0   0.0090  0.0032   R  \n",
       "1   0.0052  0.0044   R  \n",
       "2   0.0095  0.0078   R  \n",
       "3   0.0040  0.0117   R  \n",
       "4   0.0107  0.0094   R  \n",
       "5   0.0051  0.0062   R  \n",
       "6   0.0036  0.0103   R  \n",
       "7   0.0048  0.0053   R  \n",
       "8   0.0059  0.0022   R  \n",
       "9   0.0056  0.0040   R  \n",
       "10  0.0053  0.0036   R  \n",
       "11  0.0009  0.0044   R  \n",
       "\n",
       "[12 rows x 61 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\n",
    "    test_size=0.20, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB: 0.772059 (0.093962)\n",
      "GBM: 0.790809 (0.133486)\n",
      "RF: 0.717279 (0.105528)\n",
      "ET: 0.777206 (0.154037)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAJMCAYAAACPXIWpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBBCvCAkQRNCCCGEEEKIF8T/AGMIWNGBFkySAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f190fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Boosting\n",
    "\n",
    "# Boosting Method ensembles\n",
    "ensembles = []\n",
    "ensembles.append(('AB', AdaBoostClassifier())) \n",
    "ensembles.append(('GBM', GradientBoostingClassifier())) \n",
    "ensembles.append(('RF', RandomForestClassifier())) \n",
    "ensembles.append(('ET', ExtraTreesClassifier())) \n",
    "results = []\n",
    "names = []\n",
    "for name, model in ensembles:\n",
    "  kfold = KFold(n_splits=10, random_state=7)\n",
    "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)\n",
    "    \n",
    "# Compare Algorithms\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"Average Performance of Each Boosting Model with 10 KFolds on 80% (Training) Data\")\n",
    "sns.set_style(style=\"whitegrid\")\n",
    "g = sns.boxplot(data=results, palette='Set1', notch=False)\n",
    "# g.set_yticks([0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85])\n",
    "g.set_xticklabels(['AdaBoost','GradientBoosting','RandomForest','ExtraTrees'])\n",
    "g.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "### Clean up the RFC/other non-unsupervised methods and compare against MLP/Keras UL methods\n",
    "\n",
    "### Look into boosting of RFC, ADABoost, DTC -- compare with Unsupervised methods\n",
    "\n",
    "###  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Pipeline as the process from where the data were/are scraped/gathered to the actual tools to cleanse/wrangle and eventually model the data, etc.\n",
    "### Getting data from origin to polished/used in model\n",
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
