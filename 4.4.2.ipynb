{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4.4.2: Supervised NLP  \n",
    "# Kevin Hahn  \n",
    "\n",
    "## Challenge 0:  \n",
    "\n",
    "<b>Recall that the logistic regression model's best performance on the test set was 93%. See what you can do to improve performance. Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires. Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 97%.</b>  \n",
    "\n",
    "By engineering new features including counts of punctuation per sentence and word length per sentence, I was able to improve upon the logistic regression model's original performance. The punctuation count actually decreased the accuracy of the model so I excluded that feature. Using a different modeling technique, SVM Classifier, on a 80/20 train/test split, I was able to get the accuracy to 96.4%.\n",
    "\n",
    "\n",
    "## Challenge 1:  \n",
    "<b>Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.</b>  \n",
    "\n",
    "So for this challenge I loaded up another Jane Austen text, this time Sense and Sensibility. I wrote a function to use Random Forest and Logistic Regression Classifier methods to determine how good either was at identifying sentences as being from Carroll's Alice in Wonderland.  \n",
    "\n",
    "Adding the second Jane Austen text to the dataframe seemed to help increase the accuracy of the models, with training and test set scores ranging from 95.6% to 99.3% accuracy. Here are the outcomes of the RFC and Logit Classifier methods:\n",
    "\n",
    "RANDOM FOREST CLASSIFIER  \n",
    "Training set score: 0.993304816812  \n",
    "\n",
    "Test set score: 0.955648535565\n",
    "\n",
    "LOGIT REGRESSION CLASSIFIER  \n",
    "Training set score: 0.976008926911  \n",
    "\n",
    "Test set score: 0.960390516039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "import spacy\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "\n",
      "Raw:\n",
      " [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n",
      "\n",
      "Raw:\n",
      " [Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a\n",
      "\n",
      "Raw:\n",
      " [Sense and Sensibility by Jane Austen 1811]\n",
      "\n",
      "CHAPTER 1\n",
      "\n",
      "\n",
      "The family of Dashwood had long been settle\n"
     ]
    }
   ],
   "source": [
    "# Import the data we just downloaded and installed.\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "# Grab and process the raw data.\n",
    "print(gutenberg.fileids())\n",
    "\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "sense = gutenberg.raw('austen-sense.txt')\n",
    "\n",
    "# Print the first 100 characters of Alice in Wonderland.\n",
    "print('\\nRaw:\\n', alice[0:100])\n",
    "\n",
    "print('\\nRaw:\\n', emma[0:100])\n",
    "\n",
    "print('\\nRaw:\\n', sense[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title removed:\n",
      " \n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER 1\n",
      "\n",
      "\n",
      "The family of Dashwood had long been settled in Sussex.\n",
      "Their estate was large, and th\n"
     ]
    }
   ],
   "source": [
    "# This pattern matches all text between square brackets.\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "persuasion = re.sub(pattern, \"\", persuasion)\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "sense = re.sub(pattern, \"\", sense)\n",
    "\n",
    "# Print the first 100 characters of Alice again.\n",
    "print('Title removed:\\n', alice[0:100])\n",
    "print('')\n",
    "print(sense[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter headings removed:\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothin\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The family of Dashwood had long been settled in Sussex.\n",
      "Their estate was large, and their resid\n"
     ]
    }
   ],
   "source": [
    "# Now we'll match and remove chapter headings.\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "sense = re.sub(r'CHAPTER .*', '', sense)\n",
    "\n",
    "\n",
    "# Ok, what's it look like now?\n",
    "print('Chapter headings removed:\\n', alice[0:100])\n",
    "print(sense[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra whitespace removed:\n",
      " Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to\n"
     ]
    }
   ],
   "source": [
    "# Remove newlines and other extra whitespace by splitting and rejoining.\n",
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "sense = ' '.join(sense.split())\n",
    "\n",
    "# All done with cleanup? Let's see how it looks.\n",
    "print('Extra whitespace removed:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "# Here is a list of the stopwords identified by NLTK.\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# All the processing work is done here, so it may take a while.\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "sense_doc = nlp(sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alice_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 34420 tokens long\n",
      "The first three tokens are 'Alice was beginning'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the objects we've built.\n",
    "print(\"The alice_doc object is a {} object.\".format(type(alice_doc)))\n",
    "print(\"It is {} tokens long\".format(len(alice_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(alice_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(alice_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The emma_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 140800 tokens long\n",
      "The first three tokens are 'The family of'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(\"The emma_doc object is a {} object.\".format(type(sense_doc)))\n",
    "print(\"It is {} tokens long\".format(len(sense_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(sense_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(sense_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: [('the', 1524), ('and', 796), ('to', 724), ('a', 611), ('I', 534), ('it', 524), ('she', 508), ('of', 499), ('said', 453), ('Alice', 394), ('was', 363), ('in', 355), ('you', 343), ('that', 274), ('as', 245), ('her', 243), (\"n't\", 205), ('at', 202), (\"'s\", 190), ('on', 189), ('had', 184), ('with', 175), ('all', 173), ('be', 145), ('for', 139), ('but', 132), ('not', 130), ('they', 129), ('very', 126), ('little', 124), ('so', 122), ('do', 118), ('out', 116), ('this', 111), ('The', 102), ('he', 101), ('down', 99), ('is', 98), ('up', 98), ('about', 94), ('one', 94), ('his', 94), ('what', 93), ('were', 86), ('them', 86), ('like', 84), ('went', 83), ('herself', 83), ('know', 83), ('could', 82), ('would', 82), ('again', 80), ('if', 78), ('or', 75), ('thought', 74), ('did', 74), ('have', 73), ('Queen', 73), ('then', 71), ('no', 69), ('when', 69), ('time', 68), ('into', 67), ('And', 67), ('see', 66), ('there', 65), ('It', 63), ('off', 62), ('me', 61), ('King', 61), ('Turtle', 58), ('began', 57), ('its', 56), ('my', 55), (\"'m\", 55), ('Hatter', 55), ('Mock', 55), ('Gryphon', 55), ('by', 54), ('way', 54), ('quite', 53), (\"'ll\", 53), ('your', 53), ('an', 52), ('much', 51), ('say', 51), ('You', 51), ('their', 50), ('head', 49), ('thing', 49), ('some', 48), ('who', 48), ('now', 48), ('think', 47), ('go', 47), ('only', 47), ('more', 47), ('voice', 46), ('looked', 45), ('just', 45)]\n",
      "\n",
      "Persuasion: [('the', 3120), ('to', 2775), ('and', 2738), ('of', 2563), ('a', 1529), ('in', 1346), ('was', 1329), ('had', 1177), ('her', 1159), ('I', 1121), ('not', 968), ('be', 949), ('it', 856), ('that', 852), ('she', 818), ('as', 787), ('he', 736), ('for', 695), ('with', 637), ('his', 625), ('have', 583), ('but', 552), ('you', 548), ('at', 519), ('all', 517), ('Anne', 497), ('been', 496), (\"'s\", 485), ('him', 465), ('could', 444), ('were', 426), ('very', 425), ('which', 415), ('by', 410), ('is', 393), ('on', 386), ('would', 351), ('so', 338), ('She', 326), ('they', 323), ('no', 309), ('Captain', 297), ('Mrs', 291), ('from', 289), ('Elliot', 288), ('or', 274), ('their', 273), ('them', 270), ('more', 269), ('Mr', 255), ('than', 243), ('an', 238), ('must', 228), ('He', 225), ('one', 221), ('being', 220), ('Wentworth', 217), ('there', 216), ('only', 213), ('The', 209), ('much', 205), ('this', 200), ('any', 198), ('such', 196), ('when', 196), ('my', 194), ('do', 193), ('Lady', 191), ('me', 188), ('who', 186), ('should', 185), ('good', 181), ('It', 181), ('little', 175), ('what', 172), ('Charles', 166), ('might', 166), ('own', 163), ('said', 163), ('if', 162), ('will', 162), ('herself', 158), ('did', 157), ('never', 153), ('time', 151), ('think', 149), ('Russell', 148), ('Sir', 144), ('now', 144), ('can', 142), ('other', 141), ('well', 141), ('are', 141), ('Walter', 140), ('Mary', 137), ('some', 135), ('man', 133), ('again', 131), ('nothing', 131), ('Musgrove', 130)]\n",
      "\n",
      "Sense: [('to', 4058), ('the', 3859), ('of', 3562), ('and', 3334), ('her', 2428), ('a', 2041), ('I', 1984), ('in', 1901), ('was', 1842), ('it', 1555), ('she', 1329), ('be', 1302), ('not', 1301), ('that', 1296), ('for', 1231), ('as', 1177), ('you', 1032), ('had', 969), ('with', 969), ('his', 941), ('he', 891), ('have', 807), ('at', 806), ('by', 736), ('is', 724), (\"'s\", 698), ('Elinor', 679), ('on', 672), ('all', 641), ('him', 629), ('so', 614), ('which', 592), ('but', 587), ('could', 567), ('Marianne', 564), ('my', 551), ('from', 527), ('Mrs.', 526), ('would', 507), ('very', 491), ('no', 484), ('their', 463), ('them', 460), ('been', 439), ('were', 437), ('they', 427), ('me', 419), ('more', 404), ('said', 392), ('any', 389), ('this', 370), ('what', 369), ('every', 361), ('than', 360), ('will', 353), ('or', 353), ('your', 345), ('such', 339), ('an', 339), ('do', 308), ('one', 303), ('much', 286), ('can', 286), ('But', 283), ('only', 282), ('sister', 280), ('must', 279), ('own', 270), ('who', 260), ('when', 259), ('mother', 257), ('She', 257), ('Edward', 257), ('herself', 254), ('Dashwood', 252), ('if', 248), ('The', 242), ('time', 236), ('am', 233), ('Jennings', 229), ('should', 228), ('know', 228), ('are', 224), ('might', 215), ('Willoughby', 213), ('did', 211), ('there', 209), ('think', 208), ('Miss', 208), ('now', 207), ('some', 206), ('though', 204), ('He', 202), ('has', 200), ('before', 196), ('well', 189), ('It', 188), ('never', 184), ('too', 184), ('thing', 183)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Utility function to calculate how frequently words appear in the text.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "    \n",
    "# The most frequent words:\n",
    "alice_freq = word_frequencies(alice_doc).most_common(100)\n",
    "persuasion_freq = word_frequencies(persuasion_doc).most_common(100)\n",
    "sense_freq = word_frequencies(sense_doc).most_common(100)\n",
    "print('Alice:', alice_freq)\n",
    "print('')\n",
    "print('Persuasion:', persuasion_freq)\n",
    "print('')\n",
    "print('Sense:', sense_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: [('said', 453), ('Alice', 394), (\"n't\", 205), (\"'s\", 190), ('little', 124), ('like', 84), ('went', 83), ('know', 83), ('thought', 74), ('Queen', 73), ('time', 68), ('King', 61), ('Turtle', 58), ('began', 57), (\"'m\", 55), ('Hatter', 55), ('Mock', 55), ('Gryphon', 55), ('way', 54), (\"'ll\", 53), ('head', 49), ('thing', 49), ('think', 47), ('voice', 46), ('looked', 45), ('got', 45), ('Rabbit', 42), (\"'ve\", 42), ('Duchess', 42), ('round', 41), ('came', 40), ('tone', 40), ('Dormouse', 40), ('great', 39), (\"'re\", 38), ('Oh', 34), ('March', 34), ('large', 33), ('looking', 32), ('moment', 31), ('long', 31), ('Hare', 31), ('things', 30), ('right', 30), ('heard', 30), ('Mouse', 30), ('found', 29), ('door', 29), ('replied', 29), ('day', 28), ('eyes', 28), ('dear', 28), ('look', 28), ('going', 27), ('tell', 27), (\"'d\", 27), ('good', 26), ('Caterpillar', 26), ('Cat', 26), ('come', 25), ('away', 25), ('poor', 25), ('course', 25), ('soon', 24), ('wo', 24), ('shall', 23), ('took', 23), ('felt', 23), ('added', 23), ('getting', 22), ('White', 22), ('half', 22), ('wish', 21), ('find', 21), ('Come', 21), ('minute', 21), ('till', 21), ('jury', 21), ('sort', 20), ('hand', 20), ('cried', 20), ('sure', 20), ('feet', 19), ('tried', 19), ('words', 19), ('curious', 19), ('use', 18), ('wonder', 18), ('house', 18), ('end', 17), ('spoke', 17), ('tea', 17), ('eat', 17), ('question', 17), ('table', 17), ('sat', 17), ('old', 17), ('asked', 17), ('court', 17), ('ran', 16)]\n",
      "\n",
      "Persuasion: [('Anne', 497), (\"'s\", 485), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('Mr', 255), ('Wentworth', 217), ('Lady', 191), ('good', 181), ('little', 175), ('Charles', 166), ('said', 163), ('time', 151), ('think', 149), ('Russell', 148), ('Sir', 144), ('Walter', 140), ('Mary', 137), ('man', 133), ('Musgrove', 130), ('know', 127), ('Miss', 122), ('soon', 121), ('father', 117), ('great', 116), ('Louisa', 111), ('Bath', 99), ('long', 95), ('thought', 90), ('having', 90), ('Elizabeth', 88), ('better', 87), ('found', 83), ('friend', 83), ('home', 83), ('young', 83), ('sister', 82), ('like', 80), ('felt', 80), ('family', 79), ('away', 78), ('house', 78), ('Uppercross', 77), ('day', 77), ('way', 77), ('feelings', 75), ('room', 75), ('Harville', 75), ('Henrietta', 74), ('sure', 73), ('Kellynch', 72), ('come', 72), ('moment', 70), ('Benwick', 70), ('Smith', 68), ('woman', 67), ('came', 67), ('Lyme', 67), ('Clay', 66), ('present', 65), ('going', 65), ('Admiral', 65), ('heard', 65), ('mind', 64), ('happy', 63), ('party', 62), ('acquaintance', 61), ('knew', 61), ('years', 58), ('half', 58), ('seen', 58), ('morning', 58), ('Croft', 58), ('wish', 58), ('shall', 55), ('evening', 55), ('look', 54), ('hope', 53), ('ought', 52), ('Oh', 52), ('gone', 51), ('saw', 51), ('place', 50), ('till', 50), ('character', 49), ('life', 49), ('dear', 49), ('known', 48), ('left', 48), ('poor', 48), ('short', 47), ('believe', 47), ('cried', 47), ('wife', 45), ('near', 45), ('possible', 45), ('looking', 45), ('best', 44), ('leave', 44), ('certainly', 44)]\n",
      "\n",
      "Sense: [(\"'s\", 698), ('Elinor', 679), ('Marianne', 564), ('Mrs.', 526), ('said', 392), ('sister', 280), ('mother', 257), ('Edward', 257), ('Dashwood', 252), ('time', 236), ('Jennings', 229), ('know', 228), ('Willoughby', 213), ('think', 208), ('Miss', 208), ('thing', 183), ('Lucy', 182), ('soon', 179), ('Mr.', 178), ('Colonel', 173), ('good', 166), ('John', 162), ('house', 159), ('little', 157), ('great', 149), ('day', 149), ('Brandon', 143), ('sure', 133), ('heart', 126), ('Ferrars', 126), ('shall', 122), ('man', 118), ('room', 118), ('thought', 116), ('saw', 113), ('Sir', 113), ('away', 111), ('dear', 107), ('felt', 104), ('young', 103), ('Middleton', 102), ('replied', 101), ('long', 100), ('Lady', 99), ('left', 98), ('moment', 97), ('mind', 97), ('happy', 95), ('come', 94), ('kind', 94), ('world', 90), ('believe', 89), ('Barton', 89), ('came', 89), ('place', 87), ('hope', 86), ('morning', 86), ('cried', 85), ('town', 85), ('present', 84), ('brother', 83), ('family', 82), ('till', 82), ('tell', 81), ('like', 80), ('affection', 79), ('better', 78), ('heard', 78), ('love', 76), ('letter', 76), ('Palmer', 75), ('way', 74), ('feelings', 73), ('wish', 73), ('gave', 71), ('found', 71), ('immediately', 71), ('hear', 70), ('spirits', 69), ('home', 68), ('subject', 68), ('told', 68), ('person', 67), ('pleasure', 67), ('far', 67), ('feel', 66), ('woman', 65), ('happiness', 65), ('hardly', 65), ('acquaintance', 64), ('look', 64), ('comfort', 63), ('behaviour', 63), ('opinion', 62), ('Oh', 62), ('engagement', 62), ('friends', 62), ('visit', 61), ('speak', 61), ('known', 60)]\n"
     ]
    }
   ],
   "source": [
    "# Use our optional keyword argument to remove stop words.\n",
    "alice_freq = word_frequencies(alice_doc, include_stop=False).most_common(100)\n",
    "persuasion_freq = word_frequencies(persuasion_doc, include_stop=False).most_common(100)\n",
    "sense_freq = word_frequencies(sense_doc, include_stop=False).most_common(100)\n",
    "print('Alice:', alice_freq)\n",
    "print('')\n",
    "print('Persuasion:', persuasion_freq)\n",
    "print('')\n",
    "print('Sense:', sense_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Alice: {'White', 'asked', \"'ve\", 'tone', 'Duchess', 'went', 'Come', 'wo', 'door', 'spoke', 'minute', 'jury', 'Mouse', 'curious', 'find', 'head', 'Dormouse', 'March', 'sat', 'Caterpillar', 'feet', 'tea', \"'re\", 'round', 'Mock', 'getting', 'Rabbit', 'began', 'Alice', 'question', 'Queen', 'ran', 'King', 'Hare', 'wonder', \"'m\", 'Gryphon', 'use', 'took', 'Cat', \"'d\", 'tried', 'old', 'right', 'court', 'course', 'sort', 'looked', 'Turtle', 'table', 'words', 'voice', \"n't\", 'things', 'large', 'added', 'end', 'eat', 'got', 'eyes', 'hand', 'Hatter', \"'ll\"}\n",
      "63\n",
      "\n",
      "Unique to Persuasion: {'Henrietta', 'best', 'Mary', 'ought', 'Benwick', 'knew', 'Bath', 'Charles', 'Walter', 'character', 'Smith', 'friend', 'short', 'Uppercross', 'Elliot', 'Croft', 'gone', 'Elizabeth', 'Captain', 'Harville', 'leave', 'evening', 'seen', 'having', 'life', 'wife', 'Admiral', 'Anne', 'years', 'Musgrove', 'party', 'Mr', 'Louisa', 'father', 'Clay', 'Mrs', 'Wentworth', 'Kellynch', 'near', 'certainly', 'possible', 'Lyme', 'Russell'}\n",
      "43\n",
      "\n",
      "Unique to Sense: {'Brandon', 'Elinor', 'love', 'gave', 'heart', 'Ferrars', 'comfort', 'letter', 'mother', 'affection', 'opinion', 'Dashwood', 'Mrs.', 'Willoughby', 'pleasure', 'Lucy', 'visit', 'kind', 'person', 'engagement', 'told', 'hear', 'happiness', 'John', 'Middleton', 'Marianne', 'Barton', 'Mr.', 'immediately', 'hardly', 'Colonel', 'brother', 'friends', 'behaviour', 'speak', 'Jennings', 'feel', 'Edward', 'town', 'subject', 'spirits', 'world', 'far', 'Palmer'}\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "# Pull out just the text from our frequency lists.\n",
    "alice_common = [pair[0] for pair in alice_freq]\n",
    "persuasion_common = [pair[0] for pair in persuasion_freq]\n",
    "sense_common = [pair[0] for pair in sense_freq]\n",
    "\n",
    "# Use sets to find the unique values in each top ten.\n",
    "print('Unique to Alice:', set(alice_common) - set(persuasion_common) - set(sense_common))\n",
    "print(len(set(alice_common) - set(persuasion_common) - set(sense_common)))\n",
    "print('')\n",
    "print('Unique to Persuasion:', set(persuasion_common) - set(alice_common) - set(sense_common))\n",
    "print(len(set(persuasion_common) - set(alice_common) - set(sense_common)))\n",
    "print('')\n",
    "print('Unique to Sense:', set(sense_common) - set(alice_common) - set(persuasion_common))\n",
    "print(len(set(sense_common) - set(alice_common) - set(persuasion_common)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice: [('say', 476), ('alice', 396), ('be', 214), ('not', 200), ('think', 130), ('go', 130), ('little', 126), ('look', 106), ('know', 103), ('come', 97), ('like', 92), ('begin', 91), ('thing', 79), ('time', 77), ('queen', 74), ('will', 71), ('get', 67), ('king', 63), ('turtle', 60), ('head', 59), (\"'s\", 57), ('hatter', 57), ('find', 56), ('way', 55), ('mock', 55), ('gryphon', 55), ('cat', 50), ('rabbit', 49), ('voice', 49), ('hear', 48), ('mouse', 47), ('oh', 44), ('try', 44), ('good', 43), ('turn', 42), ('duchess', 42), ('tone', 42), ('large', 41), ('tell', 41), ('round', 41), ('have', 40), ('dormouse', 40), ('great', 39), ('speak', 38), ('feel', 37), ('sit', 36), ('hand', 36), ('eye', 35), ('take', 35), ('march', 35), ('reply', 34), ('ask', 33), ('long', 33), ('day', 32), ('dear', 32), ('right', 32), ('minute', 32), ('shall', 31), ('moment', 31), ('door', 31), ('grow', 31), ('hare', 31), ('white', 30), ('see', 30), ('talk', 30), ('foot', 29), ('word', 29), ('run', 28), ('poor', 27), ('caterpillar', 27), ('wonder', 26), ('cry', 26), ('remark', 26), ('soon', 25), ('away', 25), ('change', 25), ('course', 25), ('leave', 24), ('sure', 24), ('would', 24), ('add', 24), ('sort', 23), ('half', 23), ('let', 22), ('old', 22), ('hurry', 21), ('happen', 21), ('wish', 21), ('eat', 21), ('question', 21), ('wait', 21), ('child', 21), ('till', 21), ('jury', 21), ('arm', 21), ('walk', 20), ('write', 20), ('remember', 20), ('face', 20), ('repeat', 20)]\n",
      "\n",
      "Persuasion: [('anne', 497), (\"'s\", 471), ('captain', 304), ('good', 291), ('mrs', 290), ('elliot', 289), ('mr', 256), ('know', 255), ('think', 254), ('lady', 242), ('wentworth', 217), ('say', 190), ('come', 183), ('little', 176), ('time', 168), ('man', 166), ('charles', 166), ('look', 157), ('great', 151), ('sir', 149), ('russell', 148), ('go', 148), ('see', 148), ('walter', 140), ('feel', 138), ('mary', 137), ('musgrove', 130), ('miss', 127), ('find', 124), ('soon', 122), ('father', 118), ('hear', 114), ('friend', 113), ('louisa', 111), ('leave', 107), ('wish', 106), ('place', 104), ('long', 103), ('bath', 103), ('day', 101), ('room', 100), ('house', 100), ('like', 99), ('sister', 99), ('feeling', 99), ('speak', 97), ('young', 97), ('have', 92), ('elizabeth', 88), ('family', 88), ('woman', 88), ('talk', 87), ('walk', 86), ('year', 85), ('home', 83), ('way', 83), ('moment', 83), ('give', 82), ('want', 79), ('away', 78), ('uppercross', 77), ('manner', 76), ('happy', 75), ('harville', 75), ('henrietta', 74), ('kellynch', 73), ('believe', 73), ('sure', 73), ('take', 72), ('begin', 71), ('present', 71), ('party', 70), ('benwick', 70), ('admiral', 69), ('tell', 69), ('mind', 68), ('smith', 68), ('lyme', 67), ('return', 66), ('clay', 66), ('thing', 65), ('mean', 64), ('hour', 62), ('croft', 62), ('acquaintance', 61), ('love', 61), ('hope', 61), ('child', 60), ('half', 60), ('bring', 59), ('morning', 59), ('meet', 58), ('dear', 58), ('eye', 58), ('bad', 58), ('shall', 57), ('pass', 56), ('evening', 56), ('word', 55), ('visit', 55)]\n",
      "\n",
      "Sense: [(\"'s\", 687), ('elinor', 680), ('marianne', 564), ('mrs.', 526), ('say', 443), ('know', 385), ('sister', 328), ('think', 327), ('mother', 260), ('time', 260), ('edward', 258), ('dashwood', 257), ('good', 253), ('come', 231), ('jennings', 229), ('miss', 216), ('great', 215), ('willoughby', 214), ('see', 208), ('thing', 205), ('day', 194), ('feel', 184), ('lucy', 183), ('soon', 181), ('mr.', 178), ('leave', 176), ('colonel', 176), ('lady', 175), ('wish', 171), ('tell', 169), ('hear', 168), ('house', 166), ('look', 165), ('give', 164), ('john', 163), ('little', 159), ('go', 147), ('brandon', 143), ('man', 141), ('find', 141), ('sure', 134), ('believe', 131), ('shall', 130), ('heart', 129), ('speak', 129), ('ferrars', 125), ('dear', 124), ('room', 123), ('sir', 119), ('moment', 115), ('friend', 115), ('young', 113), ('long', 112), ('reply', 112), ('away', 111), ('happy', 111), ('take', 109), ('return', 106), ('hope', 104), ('year', 103), ('middleton', 103), ('place', 102), ('mind', 100), ('mean', 98), ('love', 98), ('talk', 98), ('kind', 97), ('like', 96), ('letter', 96), ('cry', 93), ('present', 91), ('affection', 91), ('world', 91), ('manner', 90), ('barton', 89), ('bring', 88), ('feeling', 87), ('suppose', 87), ('morning', 87), ('spirit', 86), ('family', 85), ('town', 85), ('brother', 84), ('marry', 83), ('till', 83), ('want', 82), ('opinion', 77), ('visit', 77), ('engagement', 77), ('eye', 77), ('way', 76), ('word', 76), ('palmer', 76), ('woman', 75), ('child', 74), ('expect', 74), ('daughter', 73), ('subject', 72), ('hour', 72), ('immediately', 71)]\n",
      "\n",
      "\n",
      "Unique to Alice: {'write', 'tone', 'door', 'minute', 'mouse', 'wait', 'jury', 'ask', 'turn', 'head', 'alice', 'hurry', 'foot', 'would', 'caterpillar', 'round', 'duchess', 'oh', 'question', 'not', 'arm', 'cat', 'rabbit', 'wonder', 'face', 'sit', 'be', 'get', 'white', 'happen', 'march', 'old', 'right', 'voice', 'course', 'sort', 'will', 'turtle', 'grow', 'mock', 'repeat', 'let', 'add', 'poor', 'dormouse', 'remark', 'change', 'hare', 'large', 'try', 'eat', 'remember', 'hand', 'gryphon', 'queen', 'run', 'hatter', 'king'}\n",
      "\n",
      "Unique to Persuasion: {'home', 'walter', 'smith', 'mrs', 'wentworth', 'musgrove', 'admiral', 'uppercross', 'benwick', 'captain', 'mr', 'meet', 'elizabeth', 'kellynch', 'evening', 'harville', 'anne', 'charles', 'clay', 'mary', 'bad', 'pass', 'elliot', 'louisa', 'party', 'father', 'acquaintance', 'henrietta', 'russell', 'lyme', 'bath', 'croft'}\n",
      "\n",
      "Uniqe to Sense: {'marianne', 'mrs.', 'dashwood', 'heart', 'letter', 'mother', 'elinor', 'affection', 'opinion', 'jennings', 'kind', 'engagement', 'spirit', 'john', 'middleton', 'daughter', 'immediately', 'edward', 'barton', 'palmer', 'brother', 'brandon', 'town', 'lucy', 'ferrars', 'suppose', 'subject', 'marry', 'willoughby', 'world', 'expect', 'colonel', 'mr.'}\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently lemmas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemmas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate our list of most common lemmas.\n",
    "alice_lemma_freq = lemma_frequencies(alice_doc, include_stop=False).most_common(100)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_doc, include_stop=False).most_common(100)\n",
    "sense_lemma_freq = lemma_frequencies(sense_doc, include_stop=False).most_common(100)\n",
    "print('\\nAlice:', alice_lemma_freq)\n",
    "print('')\n",
    "print('Persuasion:', persuasion_lemma_freq)\n",
    "print('')\n",
    "print('Sense:', sense_lemma_freq)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the other.\n",
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "sense_lemma_common = [pair[0] for pair in sense_lemma_freq]\n",
    "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common) - set(sense_lemma_common))\n",
    "print('')\n",
    "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common) - set(sense_lemma_common))\n",
    "print('')\n",
    "print('Uniqe to Sense:', set(sense_lemma_common) - set(alice_lemma_common) - set(persuasion_lemma_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice in Wonderland has 1163 sentences.\n",
      "Here is an example: \n",
      "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial exploration of sentences.\n",
    "sentences = list(alice_doc.sents)\n",
    "print(\"Alice in Wonderland has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 words in this sentence, and 25 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "# Look at some metrics around this sentence.\n",
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print((\"There are {} words in this sentence, and {} of them are\"\n",
    "       \" unique.\").format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(nlp(\"I need a break\")[3].pos_)\n",
    "print(nlp(\"I need to break the glass\")[3].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parts of speech:\n",
      "There ADV\n",
      "was VERB\n",
      "nothing NOUN\n",
      "so ADP\n",
      "VERY PROPN\n",
      "remarkable ADJ\n",
      "in ADP\n",
      "that DET\n",
      "; PUNCT\n"
     ]
    }
   ],
   "source": [
    "# View the part of speech for some tokens in our sentence.\n",
    "print('\\nParts of speech:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.pos_)\n",
    "    \n",
    "## There is acting as part of the verb, compound verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependencies:\n",
      "There expl was\n",
      "was ROOT was\n",
      "nothing attr was\n",
      "so advmod remarkable\n",
      "VERY compound remarkable\n",
      "remarkable amod nothing\n",
      "in prep remarkable\n",
      "that pobj in\n",
      "; punct was\n"
     ]
    }
   ],
   "source": [
    "# View the dependencies for some tokens.\n",
    "print('\\nDependencies:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.dep_, token.head.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Alice\n",
      "PERSON Alice\n",
      "PERSON White Rabbit\n",
      "ORG VERY\n",
      "PERSON Alice\n",
      "ORG VERY\n",
      "PERSON Rabbit\n",
      "PERSON Rabbit\n",
      "EVENT WATCH\n",
      "ORG POCKET\n",
      "\n",
      "ORG Dashwood\n",
      "GPE Sussex\n",
      "GPE Norland Park\n",
      "DATE many generations\n",
      "DATE many years\n",
      "DATE ten years\n",
      "PERSON Henry Dashwood\n",
      "GPE Norland\n",
      "NORP Gentleman\n",
      "PERSON Henry Dashwood\n"
     ]
    }
   ],
   "source": [
    "# Extract the first ten entities.\n",
    "entities = list(alice_doc.ents)[0:10]\n",
    "for entity in entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))\n",
    "\n",
    "print('')\n",
    "\n",
    "sense_entities = list(sense_doc.ents)[0:10]\n",
    "for entity in sense_entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Game', 'Conqueror', 'THIS', 'Begin', 'Majesty', 'Prizes', 'then!--Bill', 'Seaography', 'Lory', 'Soup', 'ALICE', 'Mine', 'Pigeon', 'VOICE', 'Mouse', 'Knave', 'Hjckrrh', \"W. RABBIT'\", 'Latitude', 'Mary Ann', 'Shy', 'Dormouse', 'Morcar', 'Duck', \"Rabbit's--'Pat\", 'Hush', 'Silence', 'Beautiful', 'Hand', 'Longitude', 'Mock', 'White Rabbit', 'Jack', 'Tis', 'Pat', 'Rabbit', 'Cheshire Puss', 'Alice', 'THAT', 'Mock Turtle', 'Bill', 'Soles', 'Lizard', 'Queen', 'Ahem', 'Pepper', 'Last', 'Hare', 'King', 'Tut', 'Gryphon', 'THESE', 'O Mouse', 'Cheshire Cat', 'Dinn', 'Treacle', 'WILLIAM', 'Cat', 'Footman', 'Twinkle', 'Dinah', 'Brandy', 'Edwin', 'Shakespeare', 'WHAT', 'Eaglet', 'ME', 'Drawling', 'Father William', 'Mabel', 'Edgar Atheling', 'Ma', 'ALL', 'ONE', 'Tortoise', 'Beau', 'Dodo', 'Tillie;', \"Mary Ann!'\", 'Idiot', 'Curiouser', 'William'}\n",
      "\n",
      "{'John Middleton', 'Godby', 'Brandon', 'Sally', 'Ferrars--\"very', \"Lady Middleton's\", 'her;--', 'Westons', 'Ferrars', 'Fanny', 'Absence', \"Ma'am\", 'Palmers', 'Miss Steele', 'Pope', 'Barton Park', 'HIS', 'Palmer--\"then', 'WHERE', 'Barton Cottage', 'Hope', 'Delaford', 'Robert', 'Palmer;--\"Not', 'Bishop', 'John Smith', 'Park', 'Miss Margaret', 'Long', 'Richard', 'Mama', 'Middleton', 'JOHN WILLOUGHBY', 'Barton', \"John Dashwood's\", 'Thomas Palmer', 'Lady Elliott', 'Anne', 'LUCY FERRARS', 'Richardson', 'Edward WOULD', 'Harris', 'ME', 'Steele', 'Margaret', 'Miss Marianne', 'Burgess', 'Sir Robert', 'William', 'Betty', 'Jenning', 'Confess', 'YOU.--', 'Colonel Brandon', 'Little', 'Mary', 'Dorsetshire', 'Sir John Middleton', 'Simpson', 'John Willoughby', 'ELINOR', 'Clarke', 'Eliza Williams', 'Poor Edward', 'EDWARD Ferrars', 'Harry Dashwood', 'Thunderbolts', \"Miss Dashwood's\", 'Priory', 'Misses Dashwood', 'HE', 'East Kingham Farm', 'Walker', 'Sir John', 'Marianne', 'Steeles', 'Miss Dashwood', 'Morton', 'Smith--', 'Annamaria', 'Mary Brown', 'Shakespeare', 'Grandeur', 'Biddy Henshawe', 'friend.\"-- Marianne', \"Sir John's\", 'Thomas', 'Ma', 'HER', 'Williams', 'ONE', \"Miss Morton's\", 'Esq', 'Robert,--', 'Edward Ferrars', 'Eliza', 'Palmer', 'Extravagance', 'Middletons', 'Cowper', 'Bartlett', 'Elinor', 'Steele--', 'Mamma', 'Marianne DOES', 'Martha Sharpe', \"ma'am\", 'with-- Lord', 'ROBERT', 'Columella', 'Smith', 'Willoughbys', 'Dashwood', 'Lady Middleton', 'Marianne Dashwood', 'Lucy', 'Richardsons', 'Avignon', \"Colonel Brandon's\", 'Miss Grey', 'Davies', 'Lucy Steele', 'Norland', 'Sparks', 'Harry', 'Miss Sparks', 'Marianne?--', 'Pratt', 'Park Street', 'Impudence', 'Jennings', 'Jennings--\"Oh', 'Elliott', 'Miss Williams', 'Combe Magna', 'Aye', \"Lord Morton's\", 'Bonomi', 'Folly', 'ROBERT Ferrars', 'Nancy', 'Barton cottage!--', 'Taylor', \"John Middleton's\", 'Barton Valley', 'Madam', 'Lord', 'myself--', 'Drury Lane', 'Miss Morton', 'Cold-hearted', 'Dennison', 'Willoughby', 'John Dashwood', 'Allenham', 'THAT', 'Somersetshire', 'Marianne come?\"-- Elinor', 'Gray', 'Cowper!--', 'Donavan', 'THESE', 'Gibson', 'Edward Ferrars!--I', 'John', 'Ellison', 'Scott', 'Rose', 'John.--\"It', 'Marianne NOW', 'Dashwoods', 'Michaelmas', 'Lord Courtland', 'Poor Anne', 'Grey', 'Careys', 'Longstaple', 'Edward', 'Poor Marianne', 'Queen Mab', 'Ellisons', 'Robert Ferrars', 'Henry Dashwood', 'Cartwright'}\n"
     ]
    }
   ],
   "source": [
    "# All of the uniqe entities spaCy thinks are people.\n",
    "people = [entity.text for entity in list(alice_doc.ents) if entity.label_ == \"PERSON\"]\n",
    "print(set(people))\n",
    "\n",
    "print('')\n",
    "\n",
    "sense_people = [entity.text for entity in list(sense_doc.ents) if entity.label_ == 'PERSON']\n",
    "print(set(sense_people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "sense = re.sub(r'Chapter .*', '', sense)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "sense = text_cleaner(sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !, I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((, when, she, thought, it, over, afterwards, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(In, another, moment, down, went, Alice, after...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(The, rabbit, -, hole, went, straight, on, lik...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Either, the, well, was, very, deep, ,, or, sh...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(First, ,, she, tried, to, look, down, and, ma...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(She, took, down, a, jar, from, one, of, the, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3            (Oh, dear, !, I, shall, be, late, !, ')  Carroll\n",
       "4  ((, when, she, thought, it, over, afterwards, ...  Carroll\n",
       "5  (In, another, moment, down, went, Alice, after...  Carroll\n",
       "6  (The, rabbit, -, hole, went, straight, on, lik...  Carroll\n",
       "7  (Either, the, well, was, very, deep, ,, or, sh...  Carroll\n",
       "8  (First, ,, she, tried, to, look, down, and, ma...  Carroll\n",
       "9  (She, took, down, a, jar, from, one, of, the, ...  Carroll"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "sense_sents = [[sent, \"Austen\"] for sent in sense_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents + sense_sents)\n",
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords + sensewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n",
      "Processing row 7000\n",
      "Processing row 7500\n",
      "Processing row 8000\n",
      "Processing row 8500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seventeen</th>\n",
       "      <th>surround</th>\n",
       "      <th>wretchedness</th>\n",
       "      <th>prodigious</th>\n",
       "      <th>knock</th>\n",
       "      <th>camden</th>\n",
       "      <th>widow</th>\n",
       "      <th>represent</th>\n",
       "      <th>wise</th>\n",
       "      <th>tis</th>\n",
       "      <th>...</th>\n",
       "      <th>fair</th>\n",
       "      <th>undoubtedly</th>\n",
       "      <th>strengthen</th>\n",
       "      <th>exclaim</th>\n",
       "      <th>string</th>\n",
       "      <th>execute</th>\n",
       "      <th>protection</th>\n",
       "      <th>representation</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !, I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>((, when, she, thought, it, over, afterwards, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  seventeen surround wretchedness prodigious knock camden widow represent  \\\n",
       "0         0        0            0          0     0      0     0         0   \n",
       "1         0        0            0          0     0      0     0         0   \n",
       "2         0        0            0          0     0      0     0         0   \n",
       "3         0        0            0          0     0      0     0         0   \n",
       "4         0        0            0          0     0      0     0         0   \n",
       "\n",
       "  wise tis     ...     fair undoubtedly strengthen exclaim string execute  \\\n",
       "0    0   0     ...        0           0          0       0      0       0   \n",
       "1    0   0     ...        0           0          0       0      0       0   \n",
       "2    0   0     ...        0           0          0       0      0       0   \n",
       "3    0   0     ...        0           0          0       0      0       0   \n",
       "4    0   0     ...        0           0          0       0      0       0   \n",
       "\n",
       "  protection representation  \\\n",
       "0          0              0   \n",
       "1          0              0   \n",
       "2          0              0   \n",
       "3          0              0   \n",
       "4          0              0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3            (Oh, dear, !, I, shall, be, late, !, ')     Carroll  \n",
       "4  ((, when, she, thought, it, over, afterwards, ...     Carroll  \n",
       "\n",
       "[5 rows x 3601 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.992188952948\n",
      "\n",
      "Test set score: 0.953417015342\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                Y,\n",
    "                                                test_size=0.4,\n",
    "                                                random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_ml_model(X, Y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "    train = rfc.fit(X_train, y_train)\n",
    "\n",
    "    print('RANDOM FOREST CLASSIFIER')\n",
    "    print('Training set score:', rfc.score(X_train, y_train))\n",
    "    print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "    print('\\n')\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    train = lr.fit(X_train, y_train)\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print('LOGIT REGRESSION CLASSIFIER')\n",
    "    print('Training set score:', lr.score(X_train, y_train))\n",
    "    print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5377, 3599) (5377,)\n",
      "Training set score: 0.976938813465\n",
      "\n",
      "Test set score: 0.960948396095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.954621536173\n",
      "\n",
      "Test set score: 0.951743375174\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma)\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "\n",
    "# Emma is quite long, let's cut it down to the same length as Alice.\n",
    "# emma_sents = emma_sents[0:len(alice_sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n",
      "Processing row 7000\n",
      "Processing row 7500\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.920713694907\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>7724</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>626</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen     7724       45\n",
       "Carroll     626       68"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can model it!\n",
    "# Let's use logistic regression again.\n",
    "\n",
    "# Combine the Emma sentence data with the Alice data from the test set.\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 0:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_count(sentence):\n",
    "    count = 0 \n",
    "    for token in sentence:\n",
    "        if token.is_punct:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n",
      "Processing row 7000\n",
      "Processing row 7500\n",
      "Processing row 8000\n",
      "Processing row 8500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seventeen</th>\n",
       "      <th>surround</th>\n",
       "      <th>wretchedness</th>\n",
       "      <th>prodigious</th>\n",
       "      <th>knock</th>\n",
       "      <th>camden</th>\n",
       "      <th>widow</th>\n",
       "      <th>represent</th>\n",
       "      <th>wise</th>\n",
       "      <th>tis</th>\n",
       "      <th>...</th>\n",
       "      <th>undoubtedly</th>\n",
       "      <th>strengthen</th>\n",
       "      <th>exclaim</th>\n",
       "      <th>string</th>\n",
       "      <th>execute</th>\n",
       "      <th>protection</th>\n",
       "      <th>representation</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>sents_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  seventeen surround wretchedness prodigious knock camden widow represent  \\\n",
       "0         0        0            0          0     0      0     0         0   \n",
       "1         0        0            0          0     0      0     0         0   \n",
       "2         0        0            0          0     0      0     0         0   \n",
       "\n",
       "  wise tis    ...    undoubtedly strengthen exclaim string execute protection  \\\n",
       "0    0   0    ...              0          0       0      0       0          0   \n",
       "1    0   0    ...              0          0       0      0       0          0   \n",
       "2    0   0    ...              0          0       0      0       0          0   \n",
       "\n",
       "  representation                                      text_sentence  \\\n",
       "0              0  (Alice, was, beginning, to, get, very, tired, ...   \n",
       "1              0  (So, she, was, considering, in, her, own, mind...   \n",
       "2              0  (There, was, nothing, so, VERY, remarkable, in...   \n",
       "\n",
       "  text_source sents_len  \n",
       "0     Carroll        67  \n",
       "1     Carroll        63  \n",
       "2     Carroll        33  \n",
       "\n",
       "[3 rows x 3602 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts['sents_len'] = word_counts['text_sentence'].apply(lambda x: len(x))\n",
    "# word_counts['punct_count'] = word_counts['text_sentence'].apply(lambda x: punct_count(x))\n",
    "word_counts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = word_counts.drop(['text_sentence', 'text_source'], axis=1)\n",
    "Y = word_counts['text_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X.as_matrix(), Y, test_size=0.20, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel = 'linear', tol=0.001)\n",
    "print(svm.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96374790853318459"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcome = word_counts['text_source'].apply(lambda x: 1 if x == 'Carroll' else 0)\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: text_source, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST CLASSIFIER\n",
      "Training set score: 0.993304816812\n",
      "\n",
      "Test set score: 0.955648535565\n",
      "\n",
      "\n",
      "(5377, 3601) (5377,)\n",
      "LOGIT REGRESSION CLASSIFIER\n",
      "Training set score: 0.976008926911\n",
      "\n",
      "Test set score: 0.960390516039\n"
     ]
    }
   ],
   "source": [
    "test_ml_model(X, outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts['text_source']\n",
    "\n",
    "\n",
    "outcome = 1: alice, 0: other\n",
    "          1: austen, 0: other\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice_doc emma_doc\n",
    "alice_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = alice_sents + emma_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = NewSentences[0]\n",
    "Y = NewSentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NewSentences = pd.DataFrame(alice_sents + emma_sents)\n",
    "\n",
    "X = NewSentences[0]\n",
    "Y = NewSentences[1]\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.20, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel = 'linear')\n",
    "print(svm.fit(X_train, y_train))\n",
    "# print(svm.score(X_validation, y_validation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
